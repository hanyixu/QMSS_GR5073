{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5RLgIT-Dsli1"},"source":["# Feature Selection Using Models Learned Thus Far"]},{"cell_type":"markdown","metadata":{"id":"e9Ra1zefsli_"},"source":["## Option 1: Feature selection using SelectFromModel\n","\n","`SelectFromModel` is a meta-transformer that can be used along with any estimator that has a `coef_` or `feature_importances_`\n","attribute after fitting.\n","\n","If a feature's `coef_` or `feature_importances_` value are below the provided threshold parameter, the feature is considered unimportant and is removed.\n","\n","Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument.\n","\n","Available heuristics are `mean`, `median` and float multiples of these like `0.1 * mean`."]},{"cell_type":"markdown","metadata":{"id":"CyJIRX7JsljG"},"source":["#### *Example: Fit a Random Forest model and use SelectFromModel to keep important features*"]},{"cell_type":"code","metadata":{"id":"yGXHOxYHsljI","executionInfo":{"status":"ok","timestamp":1728941924678,"user_tz":240,"elapsed":357,"user":{"displayName":"Mazen Asaad","userId":"15315554018675229727"}}},"source":["# Prepare the data\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","\n","data_url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n","data = pd.read_csv(data_url)\n","target=data[\"medv\"]\n","data=data.drop(['medv'], axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Train a Random Forest model\n","forest = RandomForestRegressor(n_estimators=200)\n","forest.fit(X_train, y_train)\n","print(forest.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAvWpsJYgsjV","executionInfo":{"status":"ok","timestamp":1728941940535,"user_tz":240,"elapsed":4560,"user":{"displayName":"Mazen Asaad","userId":"15315554018675229727"}},"outputId":"d3eee9a4-4ea1-4d9a-90f8-707aac699cdd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.03703163 0.00086692 0.00744187 0.00084376 0.01888859 0.40245067\n"," 0.01258706 0.04033267 0.00499771 0.01792048 0.02161569 0.0104349\n"," 0.42458805]\n"]}]},{"cell_type":"code","metadata":{"id":"xCHj5MzHsljc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728941957236,"user_tz":240,"elapsed":2865,"user":{"displayName":"Mazen Asaad","userId":"15315554018675229727"}},"outputId":"33a3960f-1812-4b0c-f32d-50595c5fea54"},"source":["# Use SelectFromModel with a minimum threshold of 0.25\n","sfm = SelectFromModel(forest, threshold=.25)\n","sfm.fit(X_train, y_train)\n","\n","# Transform data to select the features\n","X_train_new = sfm.transform(X_train)\n","\n","print(X_train_new[0:5,:]) #only two variables in X now\n","print(X_train_new.shape) #compare to original data with 13 variables\n","print(X_train.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 5.605 18.46 ]\n"," [ 5.927  9.22 ]\n"," [ 7.267  6.05 ]\n"," [ 6.471 17.12 ]\n"," [ 6.782 25.79 ]]\n","(379, 2)\n","(379, 13)\n"]}]},{"cell_type":"markdown","metadata":{"id":"kuUgVm8psljs"},"source":["#### *Example: Fit a Lasso model and use SelectFromModel to keep important features*"]},{"cell_type":"code","metadata":{"id":"catz5SZCslju","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728942131954,"user_tz":240,"elapsed":146,"user":{"displayName":"Mazen Asaad","userId":"15315554018675229727"}},"outputId":"00a5dbf4-6232-4220-bf35-1a354d20d3aa"},"source":["from sklearn.linear_model import Lasso\n","from sklearn.feature_selection import SelectFromModel\n","\n","las = Lasso(alpha=10).fit(X_train, y_train)\n","sfm = SelectFromModel(las)\n","sfm.fit(X_train, y_train)\n","\n","# Transform data to select the features\n","X_train_new = sfm.transform(X_train)\n","\n","print(las.coef_)\n","print(X_train_new.shape) # down from 13 variables to 4"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.          0.03268741 -0.          0.          0.          0.\n","  0.         -0.          0.         -0.01155885 -0.          0.00679306\n"," -0.54971245]\n","(379, 4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"b9evdaLislj_"},"source":["---\n","## Option 2: Use Recursive Feature Elimination\n","Given an external estimator that assigns weights to features (e.g. the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n","\n","First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_` attribute or through a `feature_importances_` attribute. Then the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n","\n","Basic algorithm:\n","1. Start by running the full model.\n","2. Run a series of models that evaluate prediction error on `y_train` after dropping a feature.\n","3. Repeat for all features.\n","4. Drop feature that is helps least in predicting `y_train`.\n","5. Repeat process with n-1 features until you reach the desired stopping criterion (e.g. target number of features)."]},{"cell_type":"markdown","source":["#### *Example:  RFE to find 5 features that lead to the best model prediction*"],"metadata":{"id":"FjJNkU07fd1s"}},{"cell_type":"code","metadata":{"id":"T9klFlhPslkC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728942350398,"user_tz":240,"elapsed":166,"user":{"displayName":"Mazen Asaad","userId":"15315554018675229727"}},"outputId":"611f8424-d8a4-4500-c37e-e5a913d6ed71"},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.feature_selection import RFE\n","\n","selector = RFE(LinearRegression(), n_features_to_select=5, step=1) # step tells RFE how many features to remove each time model features are evaluated\n","selector.fit(X_train, y_train) # fit RFE estimator\n","\n","print(\"Num Features: \"+str(selector.n_features_))\n","print(\"Feature Ranking: \"+str(selector.ranking_))  # ranking for features\n","print(\"Selected Features: \"+str(list(data.columns[selector.support_ ]))) # five most important features"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Num Features: 5\n","Feature Ranking: [3 5 9 1 1 1 8 1 4 6 1 7 2]\n","Selected Features: ['chas', 'nox', 'rm', 'dis', 'ptratio']\n"]}]},{"cell_type":"code","metadata":{"id":"wGDUZRxLslkW","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1728942352853,"user_tz":240,"elapsed":151,"user":{"displayName":"Mazen Asaad","userId":"15315554018675229727"}},"outputId":"61051121-d514-40a5-dbbd-b08d0c44ec49"},"source":["# Transform X data for other use in this model or other models\n","\n","X_train_new = selector.transform(X_train) # reduces X to subset identified above\n","display(X_train_new)"],"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([[ 0.    ,  0.431 ,  5.605 ,  7.9549, 19.1   ],\n","       [ 0.    ,  0.453 ,  5.927 ,  6.932 , 19.7   ],\n","       [ 1.    ,  0.447 ,  7.267 ,  4.7872, 17.6   ],\n","       ...,\n","       [ 0.    ,  0.547 ,  6.021 ,  2.7474, 17.8   ],\n","       [ 0.    ,  0.448 ,  6.03  ,  5.6894, 17.9   ],\n","       [ 0.    ,  0.51  ,  5.572 ,  2.5961, 16.6   ]])"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"8jtUlKeeslkl"},"source":["---\n","## Extra Practice: Can you use feature selection to transform the following dataset using different feature selection techniques?"]},{"cell_type":"code","metadata":{"id":"7SpckUUrslks","executionInfo":{"status":"ok","timestamp":1728942358903,"user_tz":240,"elapsed":292,"user":{"displayName":"Mazen Asaad","userId":"15315554018675229727"}}},"source":["from sklearn.datasets import load_breast_cancer\n","bc = load_breast_cancer()\n","\n","X = bc.data\n","y = bc.target"],"execution_count":26,"outputs":[]}]}